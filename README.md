# LLM_Evaluation_Pipeline


## Overview
A evaluation pipeline which evaluates the answer generated by the LLM and passes that answer through some parameters like cost, latency, factual accuracy etc. And in last it rates the LLM that is it working right in generating relevant answer or not. The system is built to work directly with real-world chat logs and retrieved context data without modifying the original input formats.

## Setup Instructions:
Go to Cmd powershell and follow these steps
### 1. Clone the Repository
```git clone <URL_of_the_repository_you_want_to_clone>```   to clone the repository
```cd LLM_Evaluation_Pipeline```  to go to the path of the pipeline

### 2. Creation and Activation of the Virtual Environment(Windows)
```python -m venv .venv```  to create the environment
```.venv\Scripts\activate```  to activate the environment 
once activated you gonna see (.venv) before the path name in cmd powershell

### 3. Install Dependencies
```pip install -r requirements.txt```  to install the dependencies listed in requirements.txt file

### 4. Run the Evaluation Pipeline
```python code/evaluation_pipeline.py --chat json_files/sample-chat-conversation-01.json --context json_files/sample_context_vectors-01.json --output result.json```  to run the pipeline

The result of the Evaluation_pipeline of those json input in sample_files folder will get evaluated and will be stored in the evaluation_result file in LLM_Evaluation_Pipeline folder

## Architecture of the Pipeline
The evaluation pipeline follows a simple and modular architecture:
1. Input Layer
    Chat conversation JSON (user + AI turns)
    Context retrieval JSON (vector search output)
2. Preprocessing Layer
    Extracts the latest user query
    Extracts the corresponding AI response
    Collects all retrieved context text
3. Evaluation Layer
    Response Relevance and Completeness Scoring
    Hallucination / Factual Accuracy
    Latency and Cost estimation
4. Output Layer
    Aggregates all metrics into a structured JSON output
Each stage is isolated into its own module to improve clarity and maintainability.

Firstly we ingested our 2 json files that is one context vector file and other is conversation file into the loader and the loader will ingest those file into the system. 
The loader is going to help in extracting the relevant data from the raw one and cleans the data. Then that cleaned data is sent into the evaluation layer where the data goes through all the metrics where we calculate the cosine similarity, hallucination of the content, cost to generate that answer, what the speed is? 
After than in Evaluation pipeline we connect both the loader and evaluator to generate the answer. And in output we get our answer in the range of 0 to 1(float value).

## Why we have chosen this design
This solution was intentionally designed to be simple, transparent, and computationally efficient as the primary goals. Instead of using embedding models, external APIs, or LLM-based evaluators, the pipeline relies on lightweight string similarity and context matching. This makes every metric easy to interpret and explain, which is especially important when evaluating AI behavior in a reliable and auditable manner.
Another key reason for this approach is robustness against real-world data variability. The input JSON files reflect production-style formats rather than idealized academic schemas. By adapting the code to handle these formats directly, the pipeline avoids fragile assumptions and remains usable without preprocessing or schema transformation.
More complex alternatives—such as semantic embeddings or LLM-as-a-judge methods—were deliberately avoided because they introduce higher latency, additional operational cost, dependency on external services, and reduced reproducibility. For an evaluation pipeline, correctness and clarity were prioritized over sophistication.

## Ensuring Low Latency and Low Cost at high Scale
The pipeline is designed to operate efficiently even when scaled to millions of daily evaluations.
Latency is minimized by using only in-memory operations such as string comparison and basic token counting. There are no external API calls, database queries, or model inference steps during evaluation. Each evaluation runs independently and completes in milliseconds, making it suitable for real-time or near-real-time systems.
Costs are kept low by eliminating any runtime dependence on embedding models, vector databases, or paid inference services. Token-based cost estimation is computed locally using simple arithmetic, avoiding any interaction with billing APIs or cloud services.
From a system design perspective, the pipeline is stateless and horizontally scalable. Each evaluation can be processed independently, allowing easy parallelization using worker pools or batch processing frameworks. This ensures that increasing load can be handled by scaling infrastructure rather than increasing per-request complexity or cost.

**Author:** <Gangeshwar>

